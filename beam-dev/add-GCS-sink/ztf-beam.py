#!/usr/bin/env python3
# -*- coding: UTF-8 -*-

"""``ztf-beam`` runs a Beam pipeline (via Dataflow) which
processes ZTF alerts. Currently it:

1. Listens to the Pub/Sub stream
generated by the ZTF Consumer (which runs on a Compute Engine VM,
separate from this Beam pipeline).

2. Drops the cutouts and loads the rest to a BigQuery table.

3. Filters for likely extragalactic transients; publishes a Pub/Sub
stream with alerts that pass the filter.

4. Attempts to fit likely extragalactic transients using Salt2.
When the fit is successful:
stores a figure (object's lightcurve + Salt2 fit) in a Cloud Storage bucket;
publishes a Pub/Sub stream with fit results;
loads fit results to a BigQuery table.

Usage Example
-------------

.. code-block:: python
   :linenos:

Module Documentation
--------------------
"""

import argparse
import logging
import apache_beam as beam
from apache_beam.io import BigQueryDisposition as bqdisp
from apache_beam.io import ReadFromPubSub, WriteToPubSub, WriteToBigQuery
from apache_beam.io.gcp.bigquery_tools import RetryStrategy
from apache_beam.options.pipeline_options import PipelineOptions

import beam_helpers.data_utils as dutil
from beam_helpers.filters import is_extragalactic_transient
from beam_helpers.fit_salt2 import fitSalt2


def sink_configs(PROJECTID):
    """Configuration dicts for all pipeline sinks.

    Args:
        PROJECTID (str): Google Cloud Platform project ID

    Returns:
        snkconf = {'sinktype_dataDescription': {'config_name': value, }, }
    """
    snkconf = {
            'BQ_originalAlert': {
                # 'schema': 'SCHEMA_AUTODETECT',
                'project': PROJECTID,
                'create_disposition': bqdisp.CREATE_NEVER,
                'write_disposition': bqdisp.WRITE_APPEND,
                'validate': False,
                'insert_retry_strategy': RetryStrategy.RETRY_NEVER,
                'batch_size': 5000,
            },
            'BQ_salt2': {
                'schema': 'objectId:STRING, candid:INTEGER, success:INTEGER, ncall:INTEGER, chisq:FLOAT, ndof:INTEGER, z:FLOAT, z_err:FLOAT, t0:FLOAT, t0_err:FLOAT, x0:FLOAT, x0_err:FLOAT, x1:FLOAT, x1_err:FLOAT, c:FLOAT, c_err:FLOAT, z_z_cov:FLOAT, z_t0_cov:FLOAT, z_x0_cov:FLOAT, z_x1_cov:FLOAT, z_c_cov:FLOAT, t0_z_cov:FLOAT, t0_t0_cov:FLOAT, t0_x0_cov:FLOAT, t0_x1_cov:FLOAT, t0_c_cov:FLOAT, x0_z_cov:FLOAT, x0_t0_cov:FLOAT, x0_x0_cov:FLOAT, x0_x1_cov:FLOAT, x0_c_cov:FLOAT, x1_z_cov:FLOAT, x1_t0_cov:FLOAT, x1_x0_cov:FLOAT, x1_x1_cov:FLOAT, x1_c_cov:FLOAT, c_z_cov:FLOAT, c_t0_cov:FLOAT, c_x0_cov:FLOAT, c_x1_cov:FLOAT, c_c_cov:FLOAT, plot_lc_bytes:BYTES',
                'create_disposition': bqdisp.CREATE_NEVER,
                'write_disposition': bqdisp.WRITE_APPEND,
                'insert_retry_strategy': RetryStrategy.RETRY_NEVER,
                'batch_size': 50,
            },
            'PS_generic': {
                'with_attributes': False,  # currently using bytes
                #  may want to use these in the future:
                'id_label': None,
                'timestamp_attribute': None
            },
    }

    return snkconf

def run(PROJECTID, sources, sinks, pipeline_args):
    """Runs the ZTF Beam pipeline.
    """

    pipeline_options = PipelineOptions(pipeline_args, streaming=True)
    snkconf = sink_configs(PROJECTID)

    with beam.Pipeline(options=pipeline_options) as pipeline:

        #-- Read from PS and extract data as dicts
        PSin = (pipeline | 'ReadFromPubSub' >>
                ReadFromPubSub(topic=sources['PS_ztf']))
        alertDicts = (PSin | 'ExtractAlertDict' >>
                      beam.ParDo(dutil.extractAlertDict()))
        alertDictsSC = (alertDicts | 'StripCutouts' >>
                        beam.ParDo(dutil.stripCutouts()))

        #-- Upload original alert data to BQ
        # BQ encoding error until cutouts were stripped. see StripCutouts() for more details
        # alerts with no history cannot currently be uploaded -> RETRY_NEVER
        # TODO: track deadletters, get them uploaded to bq
        bqAdDeadletters = (alertDictsSC | 'WriteToBigQuery' >>
                           WriteToBigQuery(sinks['BQ_originalAlert'],
                                           **snkconf['BQ_originalAlert'])
                          )

        #-- Filter for extragalactic transients
        adscExgalTrans = (alertDictsSC | 'filterExgalTrans' >>
                          beam.Filter(is_extragalactic_transient)
                         )
        # to PubSub
        egtPS = (adscExgalTrans | 'exgalTransFormatDictForPubSub' >>
                 beam.ParDo(dutil.formatDictForPubSub())
                )
        psEgtDeadletters = (egtPS | 'exgalTransToPubSub' >>
                            WriteToPubSub(sinks['PS_exgalTrans'],
                                          **snkconf['PS_generic'])
                           )

        #-- Fit with Salt2
        salt2Dicts = (adscExgalTrans | 'fitSalt2' >>
                      beam.ParDo(fitSalt2())
                     )
        # to BQ
        # TODO: do something with deadletters
        bqSalt2Deadletters = (salt2Dicts | 'salt2ToBQ' >>
                              WriteToBigQuery(sinks['BQ_salt2'],
                                              **snkconf['BQ_salt2'])
                             )
        # to PubSub
        # TODO: do something with deadletters
        salt2PS = (salt2Dicts | 'salt2FormatDictForPubSub' >>
                   beam.ParDo(dutil.formatDictForPubSub())
                  )
        psSalt2Deadletters = (salt2PS | 'salt2ToPubSub' >>
                              WriteToPubSub(sinks['PS_salt2'],
                                            **snkconf['PS_generic'])
                             )

if __name__ == "__main__":  # noqa
    logging.getLogger().setLevel(logging.INFO)

    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--PROJECTID",
        help="Google Cloud Platform project name.\n",
    )
    parser.add_argument(
        "--source_PS_ztf",
        help="Pub/Sub topic to read alerts from.\n"
        '"projects/<PROJECT_NAME>/topics/<TOPIC_NAME>".',
    )
    parser.add_argument(
        "--sink_BQ_originalAlert",
        help="BigQuery table to store original alert data.\n",
    )
    parser.add_argument(
        "--sink_BQ_salt2",
        help="BigQuery table to store Salt2 fits.\n",
    )
    parser.add_argument(
        "--sink_PS_exgalTrans",
        help="Pub/Sub topic to announce extragalactic transient filter.\n",
    )
    parser.add_argument(
        "--sink_PS_salt2",
        help="Pub/Sub topic to announce Salt2 fit.\n",
    )
    # parser.add_argument(
    #     "--vizierCat",
    #     default="vizier:II/246/out",
    #     help="Vizier catalog to cross-match against."
    # )

    known_args, pipeline_args = parser.parse_known_args()

    sources = {'PS_ztf': known_args.source_PS_ztf}
    sinks = {
            'BQ_originalAlert': known_args.sink_BQ_originalAlert,
            'BQ_salt2': known_args.sink_BQ_salt2,
            'PS_exgalTrans': known_args.sink_PS_exgalTrans,
            'PS_salt2': known_args.sink_PS_salt2,
    }

    run(known_args.PROJECTID, sources, sinks, pipeline_args)
